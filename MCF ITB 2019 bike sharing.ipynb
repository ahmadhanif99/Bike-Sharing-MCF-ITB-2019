{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Berikut adalah modelling yang kami lakukan. EDA dilakukan secara terpisah menggunakan aplikasi tableau. Akan tetapi ada beberapa EDA yang dimasukan dalam notebook"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Up data\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ndef get_day(day_start):\n    day_end = day_start + pd.offsets.DateOffset(hours=23)\n    return pd.date_range(day_start, day_end, freq=\"H\")\n\ndef feature_engineering(data, interpol = False):\n    \n    from datetime import datetime, timedelta, date\n    # mendapatkan fitur yang berkaitan dengan waktu dari data datetime\n    data.loc[:, 'datetime'] = pd.DatetimeIndex(data.datetime)\n    data.loc[:, 'Hour'] = data.datetime.apply(lambda x: x.hour)\n    data.loc[:, 'month'] = data.datetime.apply(lambda x: x.month)\n    data.loc[:, 'day'] = data.datetime.apply(lambda x: x.day)\n    data.loc[:, 'woy'] = data.datetime.apply(lambda x: x.week)\n    data.loc[:, 'year'] = data.datetime.apply(lambda x: x.year)\n    data.loc[:, 'dow'] = data.datetime.apply(lambda x: x.dayofweek)\n    dt = pd.DatetimeIndex(data['datetime'])\n    data.set_index(dt, inplace=True)\n\n    # tax day\n    data.loc[get_day(pd.datetime(2011, 4, 15)), \"workingday\"] = 1\n    data.loc[get_day(pd.datetime(2012, 4, 16)), \"workingday\"] = 1\n    # thanksgiving friday\n    data.loc[get_day(pd.datetime(2011, 11, 25)), \"workingday\"] = 0\n    data.loc[get_day(pd.datetime(2012, 11, 23)), \"workingday\"] = 0\n    # tax day\n    data.loc[get_day(pd.datetime(2011, 4, 15)), \"holiday\"] = 0\n    data.loc[get_day(pd.datetime(2012, 4, 16)), \"holiday\"] = 0\n\n    # thanksgiving friday\n    data.loc[get_day(pd.datetime(2011, 11, 25)), \"holiday\"] = 1 \n    data.loc[get_day(pd.datetime(2012, 11, 23)), \"holiday\"] = 1\n\n    #storms\n    data.loc[get_day(pd.datetime(2012, 5, 21)), \"holiday\"] = 1\n\n    #tornado\n    data.loc[get_day(pd.datetime(2012, 6, 1)), \"holiday\"] = 1\n    \n    dt = pd.DatetimeIndex(data['datetime'])\n    data.set_index(dt, inplace=True)\n    data['date'] = dt.date\n    \n    #disaster\n    data['disaster']=0\n    #tornado outbreak\n    data.loc[data['date']==date(2011, 4, 16),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 4, 15),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 4, 14),\"disaster\"] = 1\n      \n    #Hurricane katie\n    data.loc[data['date']==date(2011, 9, 7),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 9, 5),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 9, 6),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 9, 8),\"disaster\"] = 1\n   \n    #philipe\n    data.loc[data['date']==date(2011, 10, 1),\"disaster\"] = 1\n\n    #Hurricane Gordon\n    data.loc[data['date']==date(2012, 8, 19),\"disaster\"] = 1\n    data.loc[data['date']==date(2012, 8, 16),\"disaster\"] = 1\n    data.loc[data['date']==date(2012, 8, 17),\"disaster\"] = 1\n    data.loc[data['date']==date(2012, 8, 18),\"disaster\"] = 1\n   \n\n   \n   \n    #Hurricane Irene\n    data.loc[data['date']==date(2011, 8, 27),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 8, 25),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 8, 26),\"disaster\"] = 1\n    data.loc[data['date']==date(2011, 8, 28),\"disaster\"] = 1\n   \n    #Hurricane Ophelia\n    data.loc[data['date']==date(2011, 9, 23),\"disaster\"] = 1\n    \n    # Aggregat dari demand sepedah pada tiap season\n    data = data.join(data.groupby('season')['cnt'].sum(), on='season', rsuffix='_count')\n    \n    \n    start2011 = date(2011, 3, 13)\n    end2011 = date(2011, 11, 6)\n    start2012 = date(2012, 3, 11)\n    end2012 = date(2012, 11, 4)\n    \n    # Kondisi selain winter\n    data['non_winter'] = data['datetime'].apply(lambda x: int(((x>=start2011)& (x<end2011)) | ((x>=start2012)& (x<end2012))))\n\n    data['afternoon'] = 0 \n    data.loc[(data['Hour'] >= 11) & (data['Hour'] <= 15), 'afternoon'] = 1 \n\n    # kondisi ideal saat bersepedah (subjektif)\n    data.loc[ (data['temp'] >0.74) & (data['windspeed']<30/67), 'ideal'] = 1\n    data['ideal'] = data['ideal'].fillna(0)\n\n    # kondisi kelembapan tinggi\n    data.loc[((data['workingday'] ==1) & (data['humidity'] >= 0.6)),  'sangat_lembap'] = 1\n    data['sangat_lembap'] = data['sangat_lembap'].fillna(0)\n\n    #christmas day and others\n    data['holiday'] = data[['month', 'day', 'holiday']].apply(lambda x: (x['holiday'], 1)[x['month'] == 12 and (x['day'] in [24, 26, 31])], axis = 1)\n    data['workingday'] = data[['month', 'day', 'workingday']].apply(lambda x: (x['workingday'], 0)[x['month'] == 12 and x['day'] in [24, 31]], axis = 1)\n\n    # Membuat fitur satu hari sebelum dan sesudah liburan\n    holidays = data.loc[data['holiday'] == 1, 'datetime'].unique()\n\n    data.loc[(data['datetime'] - timedelta(days=1)).isin(holidays) |   (data['datetime'] + timedelta(days=1)).isin(holidays), 'almost_holiday'] = np.int32(1)\n    data['almost_holiday'] = data['almost_holiday'].fillna(np.int32(0))\n\n    # membuat fitur yang menunjukkan keadaan peak hour \n    con1 = (data['workingday'] ==1) & ((data['Hour'] == 8) | ((data['Hour'] == 17) | (data['Hour'] == 18)) | (data['Hour'] == 12))\n    con2 = (data['workingday'] == 0) & (data['Hour']<=19) & (data['Hour']>= 10)\n    data.loc[(con1 | con2), 'peak'] = 1\n    data['peak'] = data['peak'].fillna(0)\n\n    # fitur benar-benar bekerja yaitu saat working = 1 dan holiday = 0 \n    data.loc[((data['holiday'] ==0) & (data['workingday'] == 1)), 'really_work'] = 1 \n    data['really_work'] =  data['really_work'].fillna(0)\n\n    return data\n\n# load up data\ndef load_data(fe='sol1', interpole=False):\n    train = pd.read_csv('../input/mcf-mantapu/train.csv')\n    test = pd.read_csv('../input/mcf-mantapu/test.csv')\n    # jika tidak ingin melaukan feature engineering\n    if fe == None:    \n        return train, test \n    \n    else:\n        # jika melakukan feature engineering\n        dat =  feature_engineering(pd.concat((train, test)), interpol = interpole)\n        test = dat.loc[dat['casual'].isnull()]\n        train = dat.dropna()\n        return train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memisahkan data train dengan data test\ntrain, test = load_data(interpole=True)\nX_train = train.drop(['cnt', 'casual', 'registered'], axis=1)\ny_reg_train = np.log1p(train['registered'])\ny_cas_train = np.log1p(train['casual'])\ny_cnt_train = np.log1p(train['cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# membuat data validasi\nfrom datetime import datetime, timedelta, date\n\nval_set = pd.DataFrame(index=train.index)\nfor j in [2011, 2012]:\n    for i in (train.month.unique()):\n        val_set.loc[train.loc[(train.date > date(j, i, 17))  & train.month == i, 'cnt'].index, 'cnt'] = train.loc[(train.date > date(j, i, 17))  & train.month == i, 'cnt']\nvalindex = val_set.dropna().index\nval_set = train.loc[valindex]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data train dan validasi untuk feature selection dan parameter tuning\ntrain_fc = train.drop(valindex)\nX_train_fc = train_fc.drop(['cnt', 'casual', 'registered','date', 'datetime'], axis=1)\ny_reg_train_fc = np.log1p(train_fc['registered'])\ny_cas_train_fc = np.log1p(train_fc['casual'])\ny_cnt_train_fc = np.log1p(train_fc['cnt'])\n\nX_val = val_set.drop(['cnt', 'casual', 'registered', 'date', 'datetime'], axis=1)\ny_reg_val = np.log1p(val_set['registered'])\ny_cas_val = np.log1p(val_set['casual'])\ny_cnt_val = np.log1p(val_set['cnt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Melakukan prediksi terhadap validasi\ndef test_on_val(model, cols):\n    model.fit(X_train_fc[cols], y_reg_train_fc)\n    reg = model.predict(X_val[cols])\n    model.fit(X_train_fc[cols], y_cas_train_fc)\n    cas = model.predict(X_val[cols])\n    model.fit(X_train_fc[cols], y_cnt_train_fc)\n    cnt = model.predict(X_val[cols])\n    return reg, cas, cnt\n\n# Menghitung root mean squared error. Dikarenakan data sudah dalam bentuk log(p+1)\ndef rmse(ypred, ytrue):\n    return np.sqrt(np.mean((ytrue-ypred)**2))\n\n# algoritma untuk mencari kombinasi fitur terbaik (secara random)\ndef get_best_feature(model):\n    min_reg = 99999\n    min_cas = 99999\n    min_cnt = 99999\n    for _ in range(100):\n        cols = np.random.choice(X_train_fc.columns, size=16, replace=False)\n        rg, cs, cn = test_on_val(model, cols)\n        if rmse(rg, y_reg_val) < min_reg:\n            min_reg = rmse(rg, y_reg_val)\n            min_reg_cols = cols\n         \n        if rmse(cs, y_cas_val) < min_cas:\n            min_cas = rmse(rg, y_cas_val)\n            min_cas_cols = cols\n        \n        if rmse(cn, y_cnt_val) < min_cnt:\n            min_cnt = rmse(rg, y_cas_val)\n            min_cnt_cols = cols\n            \n    print('===========END=================')\n    print(' count cols: ', min_cnt_cols, 'score: ', min_cnt)\n    print(' regular cols: ', min_reg_cols, 'score: ', min_reg)\n    print(' casual cols: ', min_cas_cols, 'score: ', min_cas)\n    return count_cols\n# algoritma untuk melakukan hyperparameter tuning\n\ndef rand_search(cols ,n, mode_name):\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.ensemble import GradientBoostingRegressor\n\n    min_rmsle = 9999\n\n    for _ in range(n):\n        if mode_name == 'rf':\n            param_space = {\n                'n_estimators': np.random.randint(500, 1500),\n                'max_depth': np.random.randint(1, 50),\n                'min_samples_split' : np.random.randint(1, 30),\n                'n_jobs':-1,\n                'random_state': 0}\n            \n        elif mode_name == 'gb':\n            param_space = {\n                'n_estimators': np.random.randint(1000, 2000),\n                'max_depth': np.random.randint(1, 30),\n                'subsample': np.random.uniform(0.4,1),\n                'min_sample_leaf' : np.random.randint(1, 30),\n                'learning_rate':0.01,\n                'loss' : 'ls', \n                'Random_state':0}\n            \n        cur_par = param_space    \n        print('current params: ', cur_par)\n        \n        if mode_name == 'rf':\n            model = RandomForestRegressor(**cur_par)\n        elif mode_name == 'gb':\n            model = GradientBoostingRegressor(**cur_par)\n\n        cnt, cas, reg = test_on_val(model, cols)\n        pred_cnt = np.expm1(cas)+np.expm1(reg)\n        pred_cnt = np.log1p(pred_cnt)\n        curr_rmsle = rmse(pred_cnt, y_cnt_val)\n        if curr_rmsle < min_rmsle:\n            min_rmsle = curr_rmsle\n            best_pars = cur_par\n            \n    print('best rmlse: ',min_rmsle)\n    print('best par: ', cur_par)\n    \n    return best_pars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pencarian fitur serta parameter tuning memakan waktu yang sangat lama. Oleh karena itu kami tidak menyarankan untuk melakukannya lagi.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# mencari kolom terbaik \n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.ensemble import GradientBoostingRegressor\n\n# model_rf = RandomForestRegressor()\n# model_gb = GradientBoostingRegressor()\n# best_cols_rf = get_best_feature(model_rf)\n# best_cols_gb = get_best_feature(model_gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mecari paramater terbaik\n# rf_cols = [\n#     'weather', 'temp', 'atemp', 'windspeed',\n#     'workingday', 'season', 'holiday', 'sangat_lembap',\n#     'Hour', 'weekday' ,'woy','peak']\n# gbm_cols = [\n#     'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n#     'holiday', 'workingday', 'season',\n#     'Hour', 'dow', 'year', 'ideal', 'cnt_count', 'disaster',\n#     'almost_holiday', 'non_winter']\n\n# best_pars_rf = rand_search(rf_cols, 100, 'rf')\n\n\n# best_pars_rf = rand_search(gbm_cols, 100, 'gb')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hasil Prediksi terhadap validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_params = {'n_estimators': 1000, 'max_depth': 15, 'random_state': 0, 'min_samples_split' : 5, 'n_jobs': -1}\ngb_params = { 'n_estimators': 1700, \n              'max_depth': 5, \n              'random_state': 0, \n              'min_samples_leaf' : 10,\n              'learning_rate': 0.01, \n              'subsample': 0.7, \n              'loss': 'ls'}\nrf_cols = [\n    'weather', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday', 'sangat_lembap',\n    'Hour', 'weekday' ,'woy','peak']\ngbm_cols = [\n    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n    'holiday', 'workingday', 'season',\n    'Hour', 'dow', 'year', 'ideal', 'cnt_count', 'disaster',\n    'almost_holiday', 'non_winter'\n]\nrf_model = RandomForestRegressor(**rf_params)\ngbm_model = GradientBoostingRegressor(**gb_params)\n\n# predict reg\nrf_model.fit(X_train_fc[rf_cols] ,y_reg_train_fc)\ngbm_model.fit(X_train_fc[gbm_cols] ,y_reg_train_fc)\n\nreg_rf = np.expm1(rf_model.predict(X_val[rf_cols]))\nreg_gb = np.expm1(gbm_model.predict(X_val[gbm_cols]))\n\nreg_rf_wk = np.expm1(rf_model.predict(X_val.loc[X_val['workingday'] == 1, rf_cols]))\nreg_rf_nw = np.expm1(rf_model.predict(X_val.loc[X_val['workingday'] == 0, rf_cols]))\n\nreg_gb_wk = np.expm1(gbm_model.predict(X_val.loc[X_val['workingday'] == 1, gbm_cols]))\nreg_gb_nw = np.expm1(gbm_model.predict(X_val.loc[X_val['workingday'] == 0, gbm_cols]))\n\n# predict cas\nrf_model.fit(X_train_fc[rf_cols] ,y_cas_train_fc)\ngbm_model.fit(X_train_fc[gbm_cols] ,y_cas_train_fc)\n\ncas_rf = np.expm1(rf_model.predict(X_val[rf_cols]))\ncas_gb = np.expm1(gbm_model.predict(X_val[gbm_cols]))\n\ncas_rf_wk = np.expm1(rf_model.predict(X_val.loc[X_val['workingday'] == 1, rf_cols]))\ncas_rf_nw = np.expm1(rf_model.predict(X_val.loc[X_val['workingday'] == 0, rf_cols]))\n\ncas_gb_wk = np.expm1(gbm_model.predict(X_val.loc[X_val['workingday'] == 1, gbm_cols]))\ncas_gb_nw = np.expm1(gbm_model.predict(X_val.loc[X_val['workingday'] == 0, gbm_cols]))\n\n\nrf_score = rmse(np.log1p(reg_rf+cas_rf), y_cnt_val)\ngb_score = rmse(np.log1p(cas_gb+reg_gb), y_cnt_val)\n\nrf_score_casual = rmse(np.log1p(cas_rf), y_cas_val)\ngb_score_casual = rmse(np.log1p(cas_gb), y_cas_val)\n\nrf_score_registered = rmse(np.log1p(reg_rf), y_reg_val)\ngb_score_registered = rmse(np.log1p(reg_gb), y_reg_val)\n\n# the index\nwk_ind = X_val.loc[X_val['workingday'] ==1].index\nnw_ind = X_val.loc[X_val['workingday'] ==0].index\n\nrf_score_nw = rmse(np.log1p(reg_rf_nw+cas_rf_nw), y_cnt_val.loc[nw_ind])\ngb_score_nw = rmse(np.log1p(reg_gb_nw+cas_gb_nw), y_cnt_val.loc[nw_ind])\n\nrf_score_casual_nw = rmse(np.log1p(cas_rf_nw), y_cas_val.loc[nw_ind])\ngb_score_casual_nw = rmse(np.log1p(cas_gb_nw), y_cas_val.loc[nw_ind])\n\nrf_score_registered_nw = rmse(np.log1p(reg_rf_nw), y_reg_val.loc[nw_ind])\ngb_score_registered_nw = rmse(np.log1p(reg_gb_nw), y_reg_val.loc[nw_ind])\n\nrf_score_wk = rmse(np.log1p(reg_rf_wk+cas_rf_wk), y_cnt_val.loc[wk_ind])\ngb_score_wk = rmse(np.log1p(reg_gb_wk+cas_gb_wk), y_cnt_val.loc[wk_ind])\n\nrf_score_casual_wk = rmse(np.log1p(cas_rf_wk), y_cas_val.loc[wk_ind])\ngb_score_casual_wk = rmse(np.log1p(cas_gb_wk), y_cas_val.loc[wk_ind])\n\nrf_score_registered_wk = rmse(np.log1p(reg_rf_wk), y_reg_val.loc[wk_ind])\ngb_score_registered_wk = rmse(np.log1p(reg_gb_wk), y_reg_val.loc[wk_ind])\n\nprint('===================Overall====================')\n# Overall\nprint('random forest gives: ', rf_score)\nprint('gradient boosting gives: ', gb_score)\nprint('random forest registered gives: ', rf_score_registered)\nprint('gradient boosting registered gives: ', gb_score_registered)\nprint('random forest casual gives: ', rf_score_casual)\nprint('gradient boosting casual gives: ', gb_score_casual)\nprint('\\n')\n\nprint('===================Working====================')\n# Working\nprint('random forest gives: ', rf_score_wk)\nprint('gradient boosting gives: ', gb_score_wk)\nprint('random forest registered gives: ', rf_score_registered_wk)\nprint('gradient boosting registered gives: ', gb_score_registered_wk)\nprint('random forest casual gives: ', rf_score_casual_wk)\nprint('gradient boosting casual gives: ', gb_score_casual_wk)\nprint('\\n')\n\nprint('===================Non Working====================')\n# Non-Working\nprint('random forest gives: ', rf_score_nw)\nprint('gradient boosting gives: ', gb_score_nw)\nprint('random forest registered gives: ', rf_score_registered_nw)\nprint('gradient boosting registered gives: ', gb_score_registered_nw)\nprint('random forest casual gives: ', rf_score_casual_nw)\nprint('gradient boosting casual gives: ', gb_score_casual_nw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_score = 1000\nfor _ in range(300):\n    alpha = np.random.uniform(0.001, 0.99)\n    beta = 1-alpha\n    combined_score = rmse(alpha*np.log1p(reg_rf+cas_rf)+beta*np.log1p(cas_gb+reg_gb), y_cnt_val)\n    if combined_score < min_score:\n        min_score = combined_score\n        print('alpha: ', alpha)\n        print('score: ',combined_score)\n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling serta Prediksi"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest \n\n \nparams = {'n_estimators': 1000, 'max_depth': 15, 'random_state': 0, 'min_samples_split' : 5, 'n_jobs': -1}\n\n# creating the test dataset\ntest_sol2 =test.copy()\n\n# creating the models\nrf_model = RandomForestRegressor(**params)\n# used cols\nrf_cols = [\n    'weather', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday', 'sangat_lembap',\n    'Hour', 'weekday' ,'woy','peak']\n\n# fit reg\nrf_model.fit(X_train[rf_cols], y_reg_train)\n# predict predict reg\ntest_sol2['rf_registered'] = np.expm1(rf_model.predict(test[rf_cols]))\n\n#fit cas\nrf_model.fit(X_train[rf_cols], y_cas_train)\n# predict cas\ntest_sol2['rf_casual'] = np.expm1(rf_model.predict(test[rf_cols]))\n# cas+reg = cnt\ntest_sol2['rf_cnt'] = test_sol2['rf_casual'] + test_sol2['rf_registered']\n#%%\n# Gradient Booster\n\n# creating the models\nparams = {'n_estimators': 1700, \n          'max_depth': 5, \n          'random_state': 0, \n          'min_samples_leaf' : 10,\n          'learning_rate': 0.01, \n          'subsample': 0.7, \n          'loss': 'ls'}\n            \ngbm_model = GradientBoostingRegressor(**params)\ngbm_cols = [\n    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n    'holiday', 'workingday', 'season',\n    'Hour', 'dow', 'year', 'ideal', 'cnt_count', 'disaster',\n    'almost_holiday', 'non_winter'\n]\n# fit reg\ngbm_model.fit(X_train[gbm_cols], y_reg_train)\n# predict reg\ntest_sol2['gbm_registered'] = np.expm1(gbm_model.predict(test[gbm_cols]))\n# fit cas\ngbm_model.fit(X_train[gbm_cols], y_cas_train)\n#predict cas\ntest_sol2['gbm_casual'] = np.expm1(gbm_model.predict(test[gbm_cols]))\n# cas+reg = cnt\ntest_sol2['gbm_cnt'] = test_sol2['gbm_casual'] + test_sol2['gbm_registered']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Korealasi antar kolom "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Column\n# correlation between column and count\ntrain[rf_cols+['cnt']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Column\n# correlation between column and count\ntrain[gbm_cols+['cnt']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# weighted average cnt\ntest_sol2['cnt'] = (0.2*test_sol2['rf_cnt']) + (0.8*test_sol2['gbm_cnt'])\n\n\n# DISCOUNTING\n#test_sol2.loc[test_sol2['date']==date(2011, 12, 24),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2011, 12, 24),\"cnt\"]*0.6\n#test_sol2.loc[test_sol2['date']==date(2011, 12, 25),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2011, 12, 25),\"cnt\"]*0.6\n#test_sol2.loc[test_sol2['date']==date(2011, 12, 26),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2011, 12, 26),\"cnt\"]*0.6\n\n#test_sol2.loc[test_sol2['date']==date(2012, 12, 24),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2012, 12, 24),\"cnt\"]*0.6\n\n#test_sol2.loc[test_sol2['date']==date(2012, 12, 25),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2012, 12, 25),\"cnt\"]*0.6\n#test_sol2.loc[test_sol2['date']==date(2012, 12, 26),\"cnt\"] = test_sol2.loc[test_sol2['date']==date(2012, 12, 26),\"cnt\"]*0.6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export\ntest_sol2['count']=test_sol2['cnt']\ntest_sol2[['datetime', 'count']].to_csv('my_pred1.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"./my_pred1.csv\"> Download File </a>"},{"metadata":{},"cell_type":"markdown","source":"### Post Modelling Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(pd.concat((train['cnt'], test_sol2['cnt'])), color='m', label = 'demand training')\nplt.plot(test_sol2['cnt'], color='c', label ='demand prediksi')\nplt.tight_layout()\nplt.title('Hasil Prediksi')\nplt.xlabel('Datetime dalam jam')\nplt.ylabel('Demand')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_dat = pd.concat((train, test_sol2))\nplt.figure(figsize=(15,10))\nplt.scatter(x=comb_dat.groupby(comb_dat.index.date)['cnt'].mean().index, y=comb_dat.groupby(comb_dat.index.date)['cnt'].mean(), color='m', label = 'demand training')\nplt.scatter(x= test_sol2.groupby(test_sol2.index.date)['cnt'].mean().index, y = test_sol2.groupby(test_sol2.index.date)['cnt'].mean(), color='c', label='demand prediction')\nplt.tight_layout()\nplt.title('Hasil Prediksi')\nplt.xlabel('Datetime dalam hari')\nplt.ylabel('Demand')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_dat = pd.concat((train, test_sol2))\nplt.figure(figsize=(15,10))\nplt.bar(x = comb_dat.groupby(comb_dat.index.hour)['cnt'].mean().index - 0.5, height=comb_dat.groupby(comb_dat.index.hour)['cnt'].mean(), color='m', label = 'demand training',width=0.5 ,align='edge')\nplt.bar(x= comb_dat.groupby(comb_dat.index.hour)['cnt'].mean().index, height= test_sol2.groupby(test_sol2.index.hour)['cnt'].mean(),  color='c', label='demand prediction',width=0.4, align='edge')\nplt.tight_layout()\nplt.title('Hasil Prediksi')\nplt.xlabel('Jam')\nplt.ylabel('Demand')\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}